#!/bin/bash

# clsecure - Enhanced isolation with User + Namespace (Firejail)
# This file is auto-generated from modular source. Do not edit directly.
# Source: https://github.com/pablopda/clsecure
# 
# To modify: Edit files in lib/ directory and run ./build.sh

set -euo pipefail

# ============================================================================
# Library Modules (auto-merged from lib/ directory)
# ============================================================================


# --- Module: vars.sh ---
# lib/vars.sh
# 
# Global variable initialization for clsecure
# 
# Dependencies: None
# Exports: All clsecure global variables
# 
# Usage:
#   source lib/vars.sh
#   init_clsecure_vars

# Initialize all global variables for clsecure
init_clsecure_vars() {
    # Core variables
    WORKER_PREFIX="claude-worker"
    CURRENT_DIR=$(pwd)
    PROJECT_NAME=$(basename "$CURRENT_DIR")

    # Sanitize project name for username (lowercase, alphanumeric + dash)
    # Add hash suffix to avoid collisions when project names are truncated
    # Linux username limit is 32 chars: "claude-worker" (14 chars) + "-" (1) + project (max 17 chars) = 32 total
    PROJECT_NAME_SANITIZED=$(echo "$PROJECT_NAME" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9-]/-/g')
    
    # Generate short hash from full directory path to ensure uniqueness
    # Use full path to handle projects with same name in different locations
    if command -v sha256sum &>/dev/null; then
        PROJECT_HASH=$(echo -n "$CURRENT_DIR" | sha256sum | cut -c1-6)
    elif command -v shasum &>/dev/null; then
        PROJECT_HASH=$(echo -n "$CURRENT_DIR" | shasum -a 256 | cut -c1-6)
    else
        # Fallback: use first 6 chars of md5sum if available, otherwise use simple hash
        if command -v md5sum &>/dev/null; then
            PROJECT_HASH=$(echo -n "$CURRENT_DIR" | md5sum | cut -c1-6)
        else
            # Last resort: use first 6 chars of sanitized path
            PROJECT_HASH=$(echo -n "$CURRENT_DIR" | tr -cd 'a-z0-9' | cut -c1-6)
            [ -z "$PROJECT_HASH" ] && PROJECT_HASH="000000"
        fi
    fi
    
    # Combine: first 10 chars of name + dash + 6 char hash = 17 chars total
    # "claude-worker" (14) + "-" (1) + project (17) = 32 chars (fits Linux username limit)
    SAFE_PROJECT_NAME="${PROJECT_NAME_SANITIZED:0:10}-${PROJECT_HASH}"
    WORKER_USER="${WORKER_PREFIX}-${SAFE_PROJECT_NAME}"
    WORKER_HOME="/home/$WORKER_USER"
    WORKER_PROJECT="$WORKER_HOME/project"

    # Lock file
    LOCK_DIR="/tmp/claude-secure-locks"
    LOCK_FILE="$LOCK_DIR/${WORKER_USER}.lock"

    # Default isolation settings (can be overridden by config file and CLI args)
    ISOLATION_MODE="namespace"  # Options: user, namespace, container
    ALLOW_NETWORK=true
    ALLOW_DOCKER=false
    INSTALL_DEPS=false
    SETUP_SCRIPT=""
    SHELL_ONLY=false  # If true, drop into shell instead of running Claude
    SKIP_SETUP=false  # If true, skip setup script execution
    FULL_CLONE=false  # If true, clone full git history (slower)
    SESSION_NAME=""   # Session name for multiple environments per project
    SESSION_NAME_SANITIZED=""  # Sanitized session name
    CLEANUP_HOOK_TIMEOUT=30       # Timeout for project cleanup hooks (seconds)
    SKIP_DOCKER_AUTODETECT=false  # Skip docker auto-detection during cleanup

    # Config file locations (XDG standard, then fallback)
    CONFIG_FILE="${XDG_CONFIG_HOME:-$HOME/.config}/clsecure/config"
    CONFIG_FILE_ALT="$HOME/.clsecurerc"
    PROJECT_CONFIG_FILE="${CURRENT_DIR}/.clsecure/config"

    # Colors
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    BLUE='\033[0;34m'
    CYAN='\033[0;36m'
    NC='\033[0m'

    # Export variables that modules need
    export WORKER_USER WORKER_HOME WORKER_PROJECT
    export ISOLATION_MODE ALLOW_NETWORK ALLOW_DOCKER INSTALL_DEPS SETUP_SCRIPT SHELL_ONLY SKIP_SETUP FULL_CLONE SESSION_NAME SESSION_NAME_SANITIZED CLEANUP_HOOK_TIMEOUT SKIP_DOCKER_AUTODETECT
    export LOCK_FILE LOCK_DIR
    export CONFIG_FILE CONFIG_FILE_ALT PROJECT_CONFIG_FILE
    export RED GREEN YELLOW BLUE CYAN NC
    export WORKER_PREFIX CURRENT_DIR PROJECT_NAME PROJECT_NAME_SANITIZED PROJECT_HASH SAFE_PROJECT_NAME
}

# Recompute worker-related variables after SESSION_NAME is set via CLI
# Called after CLI parsing when --session is used. No-op when SESSION_NAME is empty.
# Returns 1 if session name sanitizes to empty.
recompute_worker_vars() {
    # Sanitize session name: lowercase, replace non-alnum with dash, truncate to 20 chars
    SESSION_NAME_SANITIZED=$(echo "$SESSION_NAME" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9-]/-/g' | sed 's/--*/-/g' | sed 's/^-//;s/-$//')
    SESSION_NAME_SANITIZED="${SESSION_NAME_SANITIZED:0:20}"

    if [ -z "$SESSION_NAME_SANITIZED" ]; then
        return 1
    fi

    # Recompute hash with session name appended
    local hash_input="$CURRENT_DIR:$SESSION_NAME"
    if command -v sha256sum &>/dev/null; then
        PROJECT_HASH=$(echo -n "$hash_input" | sha256sum | cut -c1-6)
    elif command -v shasum &>/dev/null; then
        PROJECT_HASH=$(echo -n "$hash_input" | shasum -a 256 | cut -c1-6)
    else
        if command -v md5sum &>/dev/null; then
            PROJECT_HASH=$(echo -n "$hash_input" | md5sum | cut -c1-6)
        else
            PROJECT_HASH=$(echo -n "$hash_input" | tr -cd 'a-z0-9' | cut -c1-6)
            [ -z "$PROJECT_HASH" ] && PROJECT_HASH="000000"
        fi
    fi

    # Recompute derived variables
    SAFE_PROJECT_NAME="${PROJECT_NAME_SANITIZED:0:10}-${PROJECT_HASH}"
    WORKER_USER="${WORKER_PREFIX}-${SAFE_PROJECT_NAME}"
    WORKER_HOME="/home/$WORKER_USER"
    WORKER_PROJECT="$WORKER_HOME/project"
    LOCK_FILE="$LOCK_DIR/${WORKER_USER}.lock"

    # Re-export changed variables
    export PROJECT_HASH SAFE_PROJECT_NAME WORKER_USER WORKER_HOME WORKER_PROJECT LOCK_FILE SESSION_NAME SESSION_NAME_SANITIZED

    return 0
}


# --- Module: logging.sh ---
# lib/logging.sh
# 
# Logging functions for clsecure
# 
# Dependencies: lib/vars.sh (for color variables)
# Exports: log_info, log_warn, log_error, log_step, log_security
# 
# Usage:
#   source lib/logging.sh
#   log_info "Message"

# Logging functions
# Note: These functions use color variables (GREEN, YELLOW, RED, BLUE, CYAN, NC)
# which are exported from lib/vars.sh. Ensure vars.sh is sourced before this module.
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[STEP]${NC} $1"; }
log_security() { echo -e "${CYAN}[SECURITY]${NC} $1"; }


# --- Module: lock.sh ---
# lib/lock.sh
# 
# Lock management for clsecure
# 
# Dependencies: lib/logging.sh, lib/vars.sh
# Exports: acquire_lock, release_lock, cleanup_on_exit
# 
# Usage:
#   source lib/lock.sh
#   acquire_lock || exit 1
#   # ... do work ...
#   release_lock

# Acquire lock atomically to prevent concurrent sessions
acquire_lock() {
    mkdir -p "$LOCK_DIR" 2>/dev/null || sudo mkdir -p "$LOCK_DIR"
    sudo chmod 1777 "$LOCK_DIR" 2>/dev/null || true

    # Use flock for atomic lock acquisition to prevent race conditions
    # Try to acquire lock with timeout of 0 (non-blocking)
    if command -v flock &>/dev/null; then
        # Use a temporary file descriptor to capture subshell exit code
        # The subshell's exit code determines if we got the lock
        if ! (
            flock -n 9 || exit 1
            # Check if existing lock file has a valid process
            if [ -f "$LOCK_FILE" ]; then
                local pid=$(cat "$LOCK_FILE" 2>/dev/null || echo "")
                if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                    exit 1
                fi
                rm -f "$LOCK_FILE" 2>/dev/null || sudo rm -f "$LOCK_FILE"
            fi
            # Write our PID to lock file
            echo $$ > "$LOCK_FILE" 2>/dev/null || echo $$ | sudo tee "$LOCK_FILE" > /dev/null
            exit 0
        ) 9>"$LOCK_FILE"; then
            # Subshell exited with non-zero (lock failed)
            return 1
        fi
        # Subshell exited with zero (lock acquired)
        return 0
    else
        # Fallback to original method if flock not available
        if [ -f "$LOCK_FILE" ]; then
            local pid=$(cat "$LOCK_FILE" 2>/dev/null || echo "")
            if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                return 1
            fi
            rm -f "$LOCK_FILE" 2>/dev/null || sudo rm -f "$LOCK_FILE"
        fi
        echo $$ > "$LOCK_FILE" 2>/dev/null || echo $$ | sudo tee "$LOCK_FILE" > /dev/null
        return 0
    fi
}

# Release lock
release_lock() {
    rm -f "$LOCK_FILE" 2>/dev/null || sudo rm -f "$LOCK_FILE" 2>/dev/null || true
}

# Cleanup handler for trap (called on exit)
cleanup_on_exit() {
    release_lock
}


# --- Module: config.sh ---
# lib/config.sh
#
# Configuration loading and validation for clsecure
#
# Dependencies: lib/logging.sh, lib/vars.sh
# Exports: load_config, show_config_info
# Internal: _parse_config_file, _trim
#
# Usage:
#   source lib/config.sh
#   load_config

# Keys that are safe to set from project-level config (allow-list).
# Any recognized key NOT in this list is rejected from project config with a warning.
# When adding new config keys, they default to user-only unless added here.
_PROJECT_SAFE_KEYS="|mode|isolation_mode|ISOLATION_MODE|cleanup_hook_timeout|CLEANUP_HOOK_TIMEOUT|skip_docker_autodetect|SKIP_DOCKER_AUTODETECT|"

# Trim leading and trailing whitespace using pure bash (no subshell/xargs).
_trim() {
    local s="$1"
    s="${s#"${s%%[![:space:]]*}"}"
    s="${s%"${s##*[![:space:]]}"}"
    printf '%s' "$s"
}

# Actionable hint for a rejected project-config key.
_project_key_hint() {
    local key="$1"
    case "$key" in
        network|allow_network|ALLOW_NETWORK)
            echo "use --allow-network/--no-network or set in ~/.config/clsecure/config" ;;
        docker|allow_docker|ALLOW_DOCKER)
            echo "use --allow-docker or set in ~/.config/clsecure/config" ;;
        setup_script|SETUP_SCRIPT)
            echo "set setup_script in ~/.config/clsecure/config" ;;
        install_dependencies|INSTALL_DEPS)
            echo "use --install-deps or set in ~/.config/clsecure/config" ;;
        *)
            echo "set in ~/.config/clsecure/config" ;;
    esac
}

# Parse a config file and apply settings.
# Arguments:
#   $1 - path to config file
#   $2 - "project" to restrict to safe keys (allow-list), "user" for all keys
_parse_config_file() {
    local config_file="$1"
    local scope="${2:-user}"

    while IFS='=' read -r key value || [ -n "$key" ]; do
        # Skip comments and empty lines
        [[ "$key" =~ ^[[:space:]]*# ]] && continue
        [[ -z "$key" ]] && continue

        # Trim whitespace (pure bash, no subshells)
        key=$(_trim "$key")
        value=$(_trim "$value")

        # Remove quotes from value if present
        value="${value#\"}"
        value="${value%\"}"
        value="${value#\'}"
        value="${value%\'}"

        # In project scope, only allow keys on the allow-list.
        # Any recognized key not on the list is warned and skipped.
        # Unrecognized keys fall through both gates harmlessly.
        if [ "$scope" = "project" ] && [[ "$_PROJECT_SAFE_KEYS" != *"|${key}|"* ]]; then
            # Only warn for keys we actually recognise (avoid noise for typos/comments)
            case "$key" in
                network|allow_network|ALLOW_NETWORK|\
                docker|allow_docker|ALLOW_DOCKER|\
                setup_script|SETUP_SCRIPT|\
                install_dependencies|INSTALL_DEPS)
                    log_warn "Project config requests '$key=$value' — ignored ($(_project_key_hint "$key"))"
                    ;;
            esac
            continue
        fi

        case "$key" in
            mode|isolation_mode|ISOLATION_MODE)
                if [[ "$value" =~ ^(user|namespace|container)$ ]]; then
                    ISOLATION_MODE="$value"
                fi
                ;;
            network|allow_network|ALLOW_NETWORK)
                if [[ "$value" =~ ^(true|false|yes|no|1|0)$ ]]; then
                    [[ "$value" =~ ^(true|yes|1)$ ]] && ALLOW_NETWORK=true || ALLOW_NETWORK=false
                fi
                ;;
            docker|allow_docker|ALLOW_DOCKER)
                if [[ "$value" =~ ^(true|false|yes|no|1|0)$ ]]; then
                    [[ "$value" =~ ^(true|yes|1)$ ]] && ALLOW_DOCKER=true || ALLOW_DOCKER=false
                fi
                ;;
            install_dependencies|INSTALL_DEPS)
                if [[ "$value" =~ ^(true|false|yes|no|1|0)$ ]]; then
                    [[ "$value" =~ ^(true|yes|1)$ ]] && INSTALL_DEPS=true || INSTALL_DEPS=false
                fi
                ;;
            setup_script|SETUP_SCRIPT)
                if [ -n "$value" ]; then
                    SETUP_SCRIPT="$value"
                fi
                ;;
            cleanup_hook_timeout|CLEANUP_HOOK_TIMEOUT)
                if [[ "$value" =~ ^[0-9]+$ ]] && [ "$value" -ge 5 ] && [ "$value" -le 300 ]; then
                    CLEANUP_HOOK_TIMEOUT="$value"
                fi
                ;;
            skip_docker_autodetect|SKIP_DOCKER_AUTODETECT)
                if [[ "$value" =~ ^(true|false|yes|no|1|0)$ ]]; then
                    [[ "$value" =~ ^(true|yes|1)$ ]] && SKIP_DOCKER_AUTODETECT=true || SKIP_DOCKER_AUTODETECT=false
                fi
                ;;
        esac
    done < "$config_file"
}

# Load configuration from project and user config files.
# Precedence: CLI flags > user config > project config > defaults
load_config() {
    # Detect misplaced project config
    if [ -f "${CURRENT_DIR}/.clsecure.conf" ] && [ ! -f "$PROJECT_CONFIG_FILE" ]; then
        log_warn "Found .clsecure.conf in project root (ignored). Use .clsecure/config instead."
    fi

    # Step 1: Load project config (safe keys only)
    if [ -f "$PROJECT_CONFIG_FILE" ]; then
        _parse_config_file "$PROJECT_CONFIG_FILE" "project"
    fi

    # Step 2: Load user config (all keys — overwrites project config values)
    local user_config=""
    if [ -f "$CONFIG_FILE" ]; then
        user_config="$CONFIG_FILE"
    elif [ -f "$CONFIG_FILE_ALT" ]; then
        user_config="$CONFIG_FILE_ALT"
    fi

    if [ -n "$user_config" ]; then
        _parse_config_file "$user_config" "user"
    fi
}

# Check if a config value is valid for its key.
# Returns 0 if valid, 1 otherwise.
_is_valid_config_value() {
    local key="$1"
    local value="$2"
    case "$key" in
        mode|isolation_mode|ISOLATION_MODE)
            [[ "$value" =~ ^(user|namespace|container)$ ]] ;;
        network|allow_network|ALLOW_NETWORK|\
        docker|allow_docker|ALLOW_DOCKER|\
        install_dependencies|INSTALL_DEPS|\
        skip_docker_autodetect|SKIP_DOCKER_AUTODETECT)
            [[ "$value" =~ ^(true|false|yes|no|1|0)$ ]] ;;
        setup_script|SETUP_SCRIPT)
            [ -n "$value" ] ;;
        cleanup_hook_timeout|CLEANUP_HOOK_TIMEOUT)
            [[ "$value" =~ ^[0-9]+$ ]] && [ "$value" -ge 5 ] && [ "$value" -le 300 ] ;;
        *)
            return 1 ;;
    esac
}

show_config_info() {
    local user_config=""
    if [ -f "$CONFIG_FILE" ]; then
        user_config="$CONFIG_FILE"
    elif [ -f "$CONFIG_FILE_ALT" ]; then
        user_config="$CONFIG_FILE_ALT"
    fi

    echo ""
    echo -e "${CYAN}╔════════════════════════════════════════╗${NC}"
    echo -e "${CYAN}║     Configuration                      ║${NC}"
    echo -e "${CYAN}╚════════════════════════════════════════╝${NC}"
    echo ""

    # Show project config
    if [ -f "$PROJECT_CONFIG_FILE" ]; then
        echo -e "${GREEN}Project config:${NC} $PROJECT_CONFIG_FILE"
        echo ""
        echo "Current contents:"
        sed 's/^/  /' "$PROJECT_CONFIG_FILE"
        echo ""
    else
        echo -e "${YELLOW}No project config found.${NC}"
        echo "  Create one at: .clsecure/config"
        echo ""
    fi

    # Show user config
    if [ -n "$user_config" ]; then
        echo -e "${GREEN}User config:${NC} $user_config"
        echo ""
        echo "Current contents:"
        sed 's/^/  /' "$user_config"
    else
        echo -e "${YELLOW}No user config found.${NC}"
        echo "  Create one at: $CONFIG_FILE"
    fi

    echo ""
    echo -e "${CYAN}Effective settings:${NC}"
    echo ""

    # Save effective values (already loaded by load_config)
    local eff_mode="$ISOLATION_MODE"
    local eff_network="$ALLOW_NETWORK"
    local eff_docker="$ALLOW_DOCKER"
    local eff_deps="$INSTALL_DEPS"
    local eff_setup="$SETUP_SCRIPT"
    local eff_cleanup_timeout="$CLEANUP_HOOK_TIMEOUT"
    local eff_skip_docker_auto="$SKIP_DOCKER_AUTODETECT"

    # Determine provenance labels
    local src_mode="default" src_network="default" src_docker="default"
    local src_deps="default" src_setup="default" src_cleanup_timeout="default"
    local src_skip_docker_auto="default"

    # Check project config for safe keys (only mark provenance if value is valid)
    if [ -f "$PROJECT_CONFIG_FILE" ]; then
        while IFS='=' read -r key value || [ -n "$key" ]; do
            [[ "$key" =~ ^[[:space:]]*# ]] && continue
            [[ -z "$key" ]] && continue
            key=$(_trim "$key")
            value=$(_trim "$value")
            value="${value#\"}" ; value="${value%\"}"
            value="${value#\'}" ; value="${value%\'}"
            # Only mark provenance for safe keys with valid values
            if [[ "$_PROJECT_SAFE_KEYS" == *"|${key}|"* ]] && _is_valid_config_value "$key" "$value"; then
                case "$key" in
                    mode|isolation_mode|ISOLATION_MODE) src_mode="project" ;;
                    cleanup_hook_timeout|CLEANUP_HOOK_TIMEOUT) src_cleanup_timeout="project" ;;
                    skip_docker_autodetect|SKIP_DOCKER_AUTODETECT) src_skip_docker_auto="project" ;;
                esac
            fi
        done < "$PROJECT_CONFIG_FILE"
    fi

    # Check user config for all keys (overrides project provenance, only if valid)
    if [ -n "$user_config" ]; then
        while IFS='=' read -r key value || [ -n "$key" ]; do
            [[ "$key" =~ ^[[:space:]]*# ]] && continue
            [[ -z "$key" ]] && continue
            key=$(_trim "$key")
            value=$(_trim "$value")
            value="${value#\"}" ; value="${value%\"}"
            value="${value#\'}" ; value="${value%\'}"
            if _is_valid_config_value "$key" "$value"; then
                case "$key" in
                    mode|isolation_mode|ISOLATION_MODE) src_mode="user" ;;
                    network|allow_network|ALLOW_NETWORK) src_network="user" ;;
                    docker|allow_docker|ALLOW_DOCKER) src_docker="user" ;;
                    install_dependencies|INSTALL_DEPS) src_deps="user" ;;
                    setup_script|SETUP_SCRIPT) src_setup="user" ;;
                    cleanup_hook_timeout|CLEANUP_HOOK_TIMEOUT) src_cleanup_timeout="user" ;;
                    skip_docker_autodetect|SKIP_DOCKER_AUTODETECT) src_skip_docker_auto="user" ;;
                esac
            fi
        done < "$user_config"
    fi

    echo "  mode = $eff_mode  [$src_mode]"
    echo "  network = $eff_network  [$src_network]"
    echo "  docker = $eff_docker  [$src_docker]"
    echo "  install_dependencies = $eff_deps  [$src_deps]"
    echo "  setup_script = ${eff_setup:-<none>}  [$src_setup]"
    echo "  cleanup_hook_timeout = $eff_cleanup_timeout  [$src_cleanup_timeout]"
    echo "  skip_docker_autodetect = $eff_skip_docker_auto  [$src_skip_docker_auto]"
    echo ""
    echo "Precedence: CLI args > User config > Project config > Defaults"
    echo ""
    echo -e "${YELLOW}Note:${NC} Project config (.clsecure/config) can only set:"
    echo "  mode, cleanup_hook_timeout, skip_docker_autodetect"
    echo ""

    exit 0
}


# --- Module: worker.sh ---
# lib/worker.sh
# 
# Worker user management for clsecure
# 
# Dependencies: lib/logging.sh, lib/config.sh, lib/vars.sh
# Exports: list_workers, cleanup_workers, cleanup_all_workers, check_worker_exists, create_worker_user, setup_worker_home
# 
# Usage:
#   source lib/worker.sh
#   create_worker_user

# List all worker users and their status
list_workers() {
    echo ""
    echo -e "${GREEN}Claude Worker Users:${NC}"
    echo ""

    local workers=$(getent passwd | grep "^${WORKER_PREFIX}-" | cut -d: -f1 || true)

    if [ -z "$workers" ]; then
        log_info "No worker users found."
        exit 0
    fi

    printf "%-30s %-10s %-15s %-15s %s\n" "USER" "STATUS" "SESSION" "SIZE" "PROJECT PATH"
    printf "%-30s %-10s %-15s %-15s %s\n" "----" "------" "-------" "----" "------------"

    for user in $workers; do
        local home_dir="/home/$user"
        local lock_file="$LOCK_DIR/${user}.lock"
        local status="idle"

        if [ -f "$lock_file" ]; then
            local pid=$(cat "$lock_file" 2>/dev/null || echo "")
            if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                status="${YELLOW}RUNNING${NC}"
            else
                sudo rm -f "$lock_file" 2>/dev/null || true
            fi
        fi

        local session="-"
        if [ -f "$home_dir/.clsecure/session_name" ]; then
            session=$(sudo cat "$home_dir/.clsecure/session_name" 2>/dev/null || echo "-")
        fi

        local size="N/A"
        if [ -d "$home_dir" ]; then
            size=$(sudo du -sh "$home_dir" 2>/dev/null | cut -f1 || echo "N/A")
        fi

        local project_path="-"
        if [ -f "$home_dir/.clsecure/project_path" ]; then
            project_path=$(sudo cat "$home_dir/.clsecure/project_path" 2>/dev/null || echo "-")
        elif [ -d "$home_dir/project/.git" ]; then
            project_path=$(sudo -u "$user" git -C "$home_dir/project" remote get-url origin 2>/dev/null | sed 's|.*/||' | sed 's|\.git$||' || echo "-")
        fi

        printf "%-30s %-10b %-15s %-15s %s\n" "$user" "$status" "$session" "$size" "$project_path"
    done

    echo ""
    exit 0
}

# Interactively cleanup specific worker users
cleanup_workers() {
    echo ""
    echo -e "${GREEN}Claude Worker Users Cleanup:${NC}"
    echo ""

    local workers=$(getent passwd | grep "^${WORKER_PREFIX}-" | cut -d: -f1 || true)

    if [ -z "$workers" ]; then
        log_info "No worker users found."
        exit 0
    fi

    local worker_array=($workers)

    echo "Found worker users:"
    for i in "${!worker_array[@]}"; do
        local user="${worker_array[$i]}"
        local home_dir="/home/$user"
        local lock_file="$LOCK_DIR/${user}.lock"
        local status=""
        local session_info=""

        if [ -f "$lock_file" ]; then
            local pid=$(cat "$lock_file" 2>/dev/null || echo "")
            if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                status=" ${YELLOW}(RUNNING - cannot remove)${NC}"
            fi
        fi

        if [ -f "$home_dir/.clsecure/session_name" ]; then
            local session=$(sudo cat "$home_dir/.clsecure/session_name" 2>/dev/null || echo "")
            [ -n "$session" ] && session_info=" [session: $session]"
        fi

        echo -e "  $((i+1))) ${user}${session_info}${status}"
    done
    echo "  q) Quit"
    echo ""

    read -p "Select user to remove (number or 'q'): " selection

    if [[ "$selection" == "q" || "$selection" == "Q" ]]; then
        exit 0
    fi

    if [[ "$selection" =~ ^[0-9]+$ ]] && [ "$selection" -ge 1 ] && [ "$selection" -le ${#worker_array[@]} ]; then
        local user="${worker_array[$((selection-1))]}"
        local lock_file="$LOCK_DIR/${user}.lock"

        if [ -f "$lock_file" ]; then
            local pid=$(cat "$lock_file" 2>/dev/null || echo "")
            if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                log_error "Cannot remove '$user' - session is still running (PID: $pid)"
                exit 1
            fi
        fi

        echo ""
        read -p "Remove user '$user' and all their files? (y/n): " confirm
        if [[ "$confirm" =~ ^[Yy]$ ]]; then
            log_info "Removing user '$user'..."
            # Run session cleanup before removing user
            WORKER_USER="$user" WORKER_HOME="/home/$user" WORKER_PROJECT="/home/$user/project" \
                cleanup_session "stop"
            sudo userdel -r "$user" 2>/dev/null || sudo userdel "$user" 2>/dev/null || true
            sudo rm -rf "/home/$user" 2>/dev/null || true
            sudo rm -f "$lock_file" 2>/dev/null || true
            log_info "Done."
        else
            log_info "Cancelled."
        fi
    else
        log_error "Invalid selection."
        exit 1
    fi

    exit 0
}

# Remove ALL worker users (requires confirmation)
cleanup_all_workers() {
    echo ""
    echo -e "${RED}WARNING: This will remove ALL claude-worker users and their files.${NC}"
    echo ""

    local workers=$(getent passwd | grep "^${WORKER_PREFIX}-" | cut -d: -f1 || true)

    if [ -z "$workers" ]; then
        log_info "No worker users found."
        exit 0
    fi

    echo "Users to be removed:"
    for user in $workers; do
        echo "  - $user"
    done
    echo ""

    read -p "Type 'DELETE ALL' to confirm: " confirm
    if [ "$confirm" = "DELETE ALL" ]; then
        for user in $workers; do
            local lock_file="$LOCK_DIR/${user}.lock"

            if [ -f "$lock_file" ]; then
                local pid=$(cat "$lock_file" 2>/dev/null || echo "")
                if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                    log_warn "Skipping '$user' - still running (PID: $pid)"
                    continue
                fi
            fi

            log_info "Removing '$user'..."
            # Run session cleanup before removing user
            WORKER_USER="$user" WORKER_HOME="/home/$user" WORKER_PROJECT="/home/$user/project" \
                cleanup_session "purge"
            sudo userdel -r "$user" 2>/dev/null || sudo userdel "$user" 2>/dev/null || true
            sudo rm -rf "/home/$user" 2>/dev/null || true
            sudo rm -f "$lock_file" 2>/dev/null || true
        done
        log_info "Done."
    else
        log_info "Cancelled."
    fi

    exit 0
}

# Check if worker user exists
check_worker_exists() {
    id "$WORKER_USER" &>/dev/null
}

# Create worker user if it doesn't exist
create_worker_user() {
    if ! check_worker_exists; then
        log_info "Creating user '$WORKER_USER'..."
        sudo useradd -m -s /bin/bash "$WORKER_USER"
        log_info "User created."
    else
        log_info "User already exists."
    fi

    # Store session metadata
    sudo mkdir -p "$WORKER_HOME/.clsecure"
    echo "$CURRENT_DIR" | sudo tee "$WORKER_HOME/.clsecure/project_path" > /dev/null
    if [ -n "${SESSION_NAME:-}" ]; then
        echo "$SESSION_NAME" | sudo tee "$WORKER_HOME/.clsecure/session_name" > /dev/null
    fi
    sudo chown -R "$WORKER_USER:$WORKER_USER" "$WORKER_HOME/.clsecure"

    # Add to docker group if docker exists and docker access is allowed
    if command -v docker &>/dev/null && [ "$ALLOW_DOCKER" = true ]; then
        if ! groups "$WORKER_USER" 2>/dev/null | grep -q '\bdocker\b'; then
            log_warn "Adding worker to docker group (grants root-equivalent access)"
            log_info "Adding to docker group..."
            sudo usermod -aG docker "$WORKER_USER"
        fi
    fi
}

# Setup worker home directory (permissions, ownership)
setup_worker_home() {
    sudo chown -R "$WORKER_USER:$WORKER_USER" "$WORKER_HOME"
    sudo chmod 755 "$WORKER_HOME"
}


# --- Module: git.sh ---
# lib/git.sh
# 
# Git operations for clsecure
# 
# Dependencies: lib/logging.sh, lib/worker.sh, lib/vars.sh
# Exports: check_disk_space, clone_repository, sync_working_directory, copy_submodules, setup_git_config
# 
# Usage:
#   source lib/git.sh
#   clone_repository

# Check available disk space before cloning
check_disk_space() {
    local required_mb="${1:-1000}"  # Default: 1GB minimum
    if command -v df &>/dev/null; then
        local available_space=$(df -m "$WORKER_HOME" 2>/dev/null | tail -1 | awk '{print $4}' || echo "0")
        if [ "$available_space" -lt "$required_mb" ] 2>/dev/null; then
            log_error "Insufficient disk space (need ${required_mb}MB, have ${available_space}MB)"
            log_info "Free up space or use a different location"
            return 1
        fi
    fi
    return 0
}

# Clone repository to worker project directory
# Uses shallow clone (50 commits) by default; set FULL_CLONE=true for full history
clone_repository() {
    sudo rm -rf "$WORKER_PROJECT" 2>/dev/null || true

    if [ "${FULL_CLONE:-false}" = true ]; then
        log_info "Cloning repository (full history)..."
        if ! sudo git clone --no-hardlinks --quiet "$CURRENT_DIR" "$WORKER_PROJECT"; then
            log_error "Failed to clone repository"
            return 1
        fi
    else
        log_info "Cloning repository (last 50 commits)..."
        # Use file:// protocol to enable --depth for local repos
        # 50 commits gives enough history for git log/blame while being fast
        if ! sudo git clone --quiet --depth 50 "file://$CURRENT_DIR" "$WORKER_PROJECT"; then
            log_error "Failed to clone repository"
            return 1
        fi
    fi
    return 0
}

# Sync working directory files (rsync)
sync_working_directory() {
    log_info "Syncing working directory..."
    sudo rsync -a \
        --exclude='.git' \
        --exclude='node_modules' \
        --exclude='venv' \
        --exclude='.venv' \
        --exclude='__pycache__' \
        --exclude='.pytest_cache' \
        --exclude='dist' \
        --exclude='build' \
        --exclude='.next' \
        --exclude='target' \
        "$CURRENT_DIR/" "$WORKER_PROJECT/"
}

# Copy submodules from source if they exist
copy_submodules() {
    # Copy submodules from source if they exist
    # This avoids needing SSH keys since we're copying from the already-cloned source
    if [ -f "$CURRENT_DIR/.gitmodules" ] && [ -d "$CURRENT_DIR/.git/modules" ]; then
        log_info "Copying submodules from source..."
        
        # Copy .git/modules directory (contains submodule git repositories)
        if ! sudo cp -r "$CURRENT_DIR/.git/modules" "$WORKER_PROJECT/.git/modules" 2>/dev/null; then
            log_warn "Failed to copy submodules - they may not work correctly"
            return 1
        fi
        
        # Fix .git pointers in submodule directories to point to local .git/modules
        # Use git config to properly read submodule paths (more robust than parsing .gitmodules)
        # Use process substitution to avoid subshell issues
        while IFS= read -r line; do
            # Line format: "submodule.<name>.path <path>"
            # Extract path value (everything after the FIRST space, not last)
            # Use read with IFS to split on first space only
            IFS=' ' read -r _ submodule_path <<< "$line"
            # Trim leading/trailing whitespace
            submodule_path="${submodule_path#"${submodule_path%%[![:space:]]*}"}"
            submodule_path="${submodule_path%"${submodule_path##*[![:space:]]}"}"
            
            if [ -n "$submodule_path" ] && [ -d "$WORKER_PROJECT/$submodule_path" ]; then
                # Git stores submodules in .git/modules using the path name
                gitdir_path="$WORKER_PROJECT/.git/modules/$submodule_path"
                if [ -d "$gitdir_path" ]; then
                    # Remove existing .git file or directory
                    sudo rm -rf "$WORKER_PROJECT/$submodule_path/.git" 2>/dev/null || true
                    # Create .git file pointing to .git/modules
                    echo "gitdir: $gitdir_path" | sudo tee "$WORKER_PROJECT/$submodule_path/.git" > /dev/null
                fi
            fi
        done < <(git config --file "$CURRENT_DIR/.gitmodules" --get-regexp '^submodule\..*\.path$' 2>/dev/null || true)
    fi
    return 0
}

# Setup git config for worker user (user.name and user.email)
setup_git_config() {
    # Read git config from the CURRENT directory (host user's repo), not from worker directory
    # This ensures we get the host user's git config, not the worker's
    # Suppress all git errors (including "fatal: failed to stat") to prevent permission issues
    local git_user_name="$(cd "$CURRENT_DIR" 2>/dev/null && git config user.name 2>/dev/null || echo "")"
    local git_user_email="$(cd "$CURRENT_DIR" 2>/dev/null && git config user.email 2>/dev/null || echo "")"

    if [ -n "$git_user_name" ] || [ -n "$git_user_email" ]; then
        # Use git config command to safely set values (avoids injection risk)
        if [ -n "$git_user_name" ]; then
            sudo -u "$WORKER_USER" git config --file "$WORKER_HOME/.gitconfig" user.name "$git_user_name" 2>/dev/null || true
        fi
        if [ -n "$git_user_email" ]; then
            sudo -u "$WORKER_USER" git config --file "$WORKER_HOME/.gitconfig" user.email "$git_user_email" 2>/dev/null || true
        fi
    fi
}


# --- Module: sanitize.sh ---
# lib/sanitize.sh
# 
# Path sanitization for MCP and Claude config files
# 
# Dependencies: lib/logging.sh, lib/worker.sh, lib/vars.sh
# Exports: sanitize_mcp_config, sanitize_worker_claude_home_paths, check_worker_mcp_runtime
# 
# Usage:
#   source lib/sanitize.sh
#   sanitize_mcp_config

# Sanitize project-local MCP config so it works under worker users.
# Claude often stores absolute tool paths (e.g. /home/<user>/.nvm/.../npx) in `.mcp.json`,
# which breaks when the repo is copied to a different Linux user/home.
sanitize_mcp_config() {
    local mcp_file="$WORKER_PROJECT/.mcp.json"

    [ -f "$mcp_file" ] || return 0

    if ! command -v python3 &>/dev/null; then
        log_warn "Found .mcp.json but python3 is unavailable; skipping MCP path sanitization."
        return 0
    fi

    log_step "Sanitizing MCP config paths (.mcp.json)..."

    # Run as the worker user so ownership/permissions stay correct.
    # Wrap in error handling so failures don't crash the main script
    sudo -u "$WORKER_USER" WORKER_PROJECT="$WORKER_PROJECT" python3 - <<'PY' || true
import json
import os
import pathlib
import sys

try:
    mcp_path = pathlib.Path(os.environ["WORKER_PROJECT"]) / ".mcp.json"

    try:
        raw = mcp_path.read_text(encoding="utf-8")
    except FileNotFoundError:
        sys.exit(0)

    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        # Don't brick the user's file; just leave it untouched.
        sys.exit(0)

    servers = data.get("mcpServers")
    if not isinstance(servers, dict):
        sys.exit(0)

    def should_portabilize(cmd: str) -> bool:
        if not cmd:
            return False
        # Common "copied-from-host-user" patterns. We keep this conservative to avoid
        # breaking legitimate custom absolute binaries.
        if "/.nvm/" in cmd:
            return True
        if "/.asdf/" in cmd:
            return True
        if "/.volta/" in cmd:
            return True
        return False

    portable_basenames = {"npx", "node", "python", "python3", "uvx", "pipx"}
    changed = False

    for name, cfg in servers.items():
        if not isinstance(cfg, dict):
            continue
        cmd = cfg.get("command")
        if not isinstance(cmd, str):
            continue

        base = os.path.basename(cmd)
        if base in portable_basenames and (os.path.isabs(cmd) and should_portabilize(cmd)):
            cfg["command"] = base
            changed = True

    if not changed:
        sys.exit(0)

    try:
        mcp_path.write_text(json.dumps(data, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
    except (OSError, PermissionError, IOError):
        # Best-effort operation: if write fails (permission denied, disk full, etc.), 
        # exit successfully so the main script continues
        sys.exit(0)
    except Exception:
        # Catch any other unexpected errors
        sys.exit(0)
except Exception:
    # Top-level exception handler: catch any unexpected errors and exit successfully
    # This is a best-effort operation and should not crash the main script
    sys.exit(0)
PY
}

check_worker_mcp_runtime() {
    # MCP servers that run via npx need node+npx available to the worker user.
    if sudo -u "$WORKER_USER" bash -c "source '$WORKER_HOME/.bashrc' >/dev/null 2>&1; command -v npx >/dev/null 2>&1"; then
        return 0
    fi

    log_warn "Worker cannot find 'npx' on PATH. MCP servers that use npx will fail to start."
    log_info "Fix options (pick one):"
    log_info "  - Install Node globally (recommended with this setup): sudo -H -u linuxbrew /home/linuxbrew/.linuxbrew/bin/brew install node"
    log_info "  - Or install Node system-wide: sudo apt-get install -y nodejs npm"
    log_info "  - Or use clsecure's setup_script hook to install Node inside the worker user (e.g. via nvm in \$HOME)"
}

sanitize_worker_claude_home_paths() {
    # After copying ~/.claude from the host user to the worker, rewrite any absolute
    # host-home paths (e.g. /home/arkat/...) to the worker home. Some Claude settings,
    # including skill/config locations, can be stored as absolute paths.
    local claude_dir="$WORKER_HOME/.claude"
    local host_home="$HOME"
    local worker_home="$WORKER_HOME"

    [ -d "$claude_dir" ] || return 0

    if ! command -v python3 &>/dev/null; then
        log_warn "python3 unavailable; skipping Claude config path sanitization."
        return 0
    fi

    log_step "Sanitizing Claude config paths (~/.claude)..."

    # Wrap in error handling so failures don't crash the main script
    sudo -u "$WORKER_USER" HOST_HOME="$host_home" WORKER_HOME="$worker_home" CLAUDE_DIR="$claude_dir" python3 - <<'PY' || true
import os
import pathlib
import re
import sys

try:
    host_home = os.environ["HOST_HOME"]
    worker_home = os.environ["WORKER_HOME"]
    claude_dir = pathlib.Path(os.environ["CLAUDE_DIR"])

    def is_probably_text(b: bytes) -> bool:
        # NUL byte is a strong signal of binary
        return b.find(b"\x00") == -1

    def replace_path_safely(text: str, old_path: str, new_path: str) -> str:
        """Replace old_path with new_path only when it appears as a standalone file path.
        
        This avoids false positives in URLs, usernames, or other non-path contexts.
        Matches when old_path is:
        - At start of string or preceded by: whitespace, quotes, =, :, [, {
        - Followed by: / (path continuation), end of string, whitespace, quotes, comma, }, ]
        """
        # Escape special regex characters in the path
        escaped_old = re.escape(old_path)
        # Pattern: match path when it appears in path-like context
        # Preceded by start/whitespace/quotes/operators, followed by / or end/whitespace/punctuation
        pattern = r'(^|[\s"\'=:\[{])' + escaped_old + r'(/|$|[\s"\'},])'
        
        def replacer(match):
            prefix = match.group(1)
            suffix = match.group(2)
            # If suffix is /, preserve it; otherwise it's end/whitespace/punctuation (preserve as-is)
            if suffix == '/':
                return prefix + new_path + '/'
            elif suffix == '':
                # End of string
                return prefix + new_path
            else:
                # Whitespace or punctuation - preserve it
                return prefix + new_path + suffix
        
        return re.sub(pattern, replacer, text)

    # Use a list to track changed files count (mutable object for nested functions)
    changed_files = [0]

    # Use iterdir() and manual recursion instead of rglob() to avoid following symlinks
    # rglob() follows symlinks by default, which can cause permission errors
    def process_directory(dir_path):
        """Recursively process directory, skipping symlinks."""
        try:
            for item in dir_path.iterdir():
                # Skip symlinks entirely to avoid following them outside worker directory
                if item.is_symlink():
                    continue
                
                if item.is_dir():
                    # Recursively process subdirectories
                    process_directory(item)
                elif item.is_file():
                    # Process the file (error handling is inside process_file)
                    process_file(item)
        except (PermissionError, OSError):
            # Skip directories we can't access
            return
    
    def process_file(p):
        """Process a single file for path sanitization."""
        try:
            st = p.stat()
        except (FileNotFoundError, PermissionError, OSError):
            return

        # Avoid large/unexpected files
        if st.st_size > 1024 * 1024:
            return

        try:
            raw = p.read_bytes()
        except (OSError, FileNotFoundError, PermissionError):
            return

        if not is_probably_text(raw):
            return

        try:
            text = raw.decode("utf-8")
        except UnicodeDecodeError:
            return

        if host_home not in text:
            return

        new_text = replace_path_safely(text, host_home, worker_home)
        if new_text != text:
            try:
                p.write_text(new_text, encoding="utf-8")
                changed_files[0] += 1
            except (OSError, PermissionError, IOError):
                # Best-effort operation: skip this file but continue processing others
                return
            except Exception:
                # Catch any other unexpected errors for this file
                return
    
    # Start processing from the claude directory
    process_directory(claude_dir)

    # Always exit successfully - this is a best-effort operation
    sys.exit(0)
except Exception:
    # Top-level exception handler: catch any unexpected errors and exit successfully
    # This is a best-effort operation and should not crash the main script
    sys.exit(0)
PY
}


# --- Module: deps.sh ---
# lib/deps.sh
#
# Dependency installation for clsecure
#
# Dependencies: lib/logging.sh, lib/worker.sh, lib/vars.sh
# Exports: install_project_dependencies, run_setup_script, copy_npm_cache, install_task_master
#
# Usage:
#   source lib/deps.sh
#   install_project_dependencies

# Copy npm cache from invoking user to worker user
# This speeds up npm installs significantly by avoiding re-downloads
copy_npm_cache() {
    local source_cache="${HOME}/.npm"
    local dest_cache="${WORKER_HOME}/.npm"

    if [ ! -d "$source_cache" ]; then
        log_info "No npm cache found at $source_cache, skipping cache copy."
        return 0
    fi

    if [ -d "$dest_cache" ]; then
        log_info "Worker npm cache already exists."
        return 0
    fi

    log_info "Copying npm cache to worker user..."
    sudo cp -r "$source_cache" "$dest_cache"
    sudo chown -R "$WORKER_USER:$WORKER_USER" "$dest_cache"
    log_info "npm cache copied successfully."
}

# Install task-master-ai with retry logic
# Args: $1 = max retries (default 2)
install_task_master() {
    local max_retries="${1:-2}"
    local attempt=1

    log_step "Checking task-master-ai..."

    # Check if already installed
    if sudo -u "$WORKER_USER" bash -c "cd && source ~/.bashrc && command -v task-master" &>/dev/null; then
        log_info "task-master-ai already installed."
        return 0
    fi

    # Copy npm cache first to speed up installation
    copy_npm_cache

    while [ $attempt -le $max_retries ]; do
        log_info "Installing task-master-ai (attempt $attempt/$max_retries)..."

        # Install with increased timeout (5 minutes)
        if sudo -u "$WORKER_USER" bash -c "cd && source ~/.bashrc && npm install -g task-master-ai --fetch-timeout=300000 --fetch-retries=3" 2>&1; then
            log_info "task-master-ai installed successfully."
            return 0
        fi

        log_warn "Attempt $attempt failed."
        attempt=$((attempt + 1))

        if [ $attempt -le $max_retries ]; then
            log_info "Retrying in 5 seconds..."
            sleep 5
        fi
    done

    log_warn "Failed to install task-master-ai after $max_retries attempts. Continuing anyway..."
    return 1
}

# Install project dependencies (npm/pip)
install_project_dependencies() {
    log_step "Installing project dependencies..."
    
    if [ -f "$WORKER_PROJECT/package.json" ]; then
        log_info "Found package.json, running npm install..."
        sudo -u "$WORKER_USER" bash -c "cd '$WORKER_PROJECT' && source \"$WORKER_HOME/.bashrc\" && npm install"
    fi

    if [ -f "$WORKER_PROJECT/requirements.txt" ]; then
        log_info "Found requirements.txt..."
        if [ ! -d "$WORKER_PROJECT/venv" ] && [ ! -d "$WORKER_PROJECT/.venv" ]; then
            log_info "Creating virtual environment..."
            sudo -u "$WORKER_USER" bash -c "cd '$WORKER_PROJECT' && python3 -m venv venv"
        fi
        
        local venv_dir="$WORKER_PROJECT/venv"
        if [ -d "$WORKER_PROJECT/.venv" ]; then
            venv_dir="$WORKER_PROJECT/.venv"
        fi
        
        log_info "Installing pip requirements in $venv_dir..."
        sudo -u "$WORKER_USER" bash -c "source '$venv_dir/bin/activate' && pip install -r '$WORKER_PROJECT/requirements.txt'"
    fi
}

# Run setup script if configured
run_setup_script() {
    if [ -z "$SETUP_SCRIPT" ]; then
        return 0
    fi

    log_step "Running setup script..."
    if [ ! -f "$SETUP_SCRIPT" ]; then
        log_warn "Setup script configured but not found: $SETUP_SCRIPT"
        return 1
    fi

    local worker_setup_script="$WORKER_HOME/setup_script.sh"
    sudo cp "$SETUP_SCRIPT" "$worker_setup_script"
    sudo chown "$WORKER_USER:$WORKER_USER" "$worker_setup_script"
    sudo chmod +x "$worker_setup_script"
    
    # Capture GH_TOKEN if available
    # Priority: Env var -> gh auth token
    local gh_token_val="${GH_TOKEN:-}"
    if [ -z "$gh_token_val" ] && command -v gh &>/dev/null; then
         gh_token_val=$(gh auth token 2>/dev/null || echo "")
    fi

    log_info "Executing $SETUP_SCRIPT..."
    if [ -n "$gh_token_val" ]; then
         if sudo -u "$WORKER_USER" GH_TOKEN="$gh_token_val" bash -c "cd && source ~/.bashrc && $worker_setup_script"; then
             log_info "Setup script executed successfully."
             return 0
         else
             log_warn "Setup script failed."
             return 1
         fi
    else
         if sudo -u "$WORKER_USER" bash -c "cd && source ~/.bashrc && $worker_setup_script"; then
             log_info "Setup script executed successfully."
             return 0
         else
             log_warn "Setup script failed."
             return 1
         fi
    fi
}


# --- Module: isolation.sh ---
# lib/isolation.sh
# 
# Isolation mode execution for clsecure
# 
# Dependencies: lib/logging.sh, lib/config.sh, lib/worker.sh, lib/vars.sh
# Exports: check_isolation_requirements, show_isolation_info, start_user_session, start_namespace_session, start_container_session
# 
# Usage:
#   source lib/isolation.sh
#   check_isolation_requirements

# Check isolation requirements (firejail/podman)
check_isolation_requirements() {
    case $ISOLATION_MODE in
        namespace)
            if ! command -v firejail &>/dev/null; then
                log_error "Firejail not found. Install with: sudo apt install firejail"
                log_info "Or use --mode user for basic isolation"
                exit 1
            fi
            log_security "Namespace isolation enabled (firejail)"
            ;;
        container)
            if ! command -v podman &>/dev/null; then
                log_error "Podman not found. Install with: sudo apt install podman"
                log_info "Or use --mode namespace for firejail isolation"
                exit 1
            fi
            log_security "Container isolation enabled (podman)"
            ;;
        user)
            log_security "User isolation enabled (basic)"
            ;;
    esac
}

# Show isolation information
show_isolation_info() {
    echo ""
    echo -e "${CYAN}╔════════════════════════════════════════╗${NC}"
    echo -e "${CYAN}║     Isolation Configuration            ║${NC}"
    echo -e "${CYAN}╚════════════════════════════════════════╝${NC}"
    echo ""

    echo -e "${GREEN}Current Mode:${NC} $ISOLATION_MODE"
    echo -e "${GREEN}Network:${NC} $([ "$ALLOW_NETWORK" = true ] && echo "Enabled" || echo "Disabled (--net=none)")"
    echo -e "${GREEN}Docker:${NC} $([ "$ALLOW_DOCKER" = true ] && echo "Enabled" || echo "Disabled")"
    echo -e "${GREEN}Install Deps:${NC} $([ "$INSTALL_DEPS" = true ] && echo "Enabled" || echo "Disabled")"
    if [ -n "$SETUP_SCRIPT" ]; then
        echo -e "${GREEN}Setup Script:${NC} $SETUP_SCRIPT"
    fi
    echo ""

    case $ISOLATION_MODE in
        user)
            echo -e "${YELLOW}User Isolation Only${NC}"
            echo "  ✓ Dedicated user: $WORKER_USER"
            echo "  ✓ Separate home directory"
            echo "  ✓ File system permissions"
            echo "  ✗ No namespace isolation"
            echo "  ✗ No capability restrictions"
            echo ""
            echo -e "${YELLOW}Security Level: 6/10${NC}"
            echo "Good for: Regular development on trusted code"
            ;;
        namespace)
            echo -e "${GREEN}User + Namespace Isolation (Recommended)${NC}"
            echo "  ✓ Dedicated user: $WORKER_USER"
            echo "  ✓ Separate home directory"
            echo "  ✓ Firejail sandbox"
            echo "  ✓ Network isolation (unless --allow-network)"
            echo "  ✓ PID namespace (process isolation)"
            echo "  ✓ Mount namespace (filesystem isolation)"
            echo "  ✓ Capability dropping (no CAP_SYS_ADMIN, etc.)"
            echo "  ✓ Seccomp filters (blocks dangerous syscalls)"
            echo "  ✓ Device isolation (no /dev/video, /dev/audio)"
            if [ "$ALLOW_DOCKER" = true ]; then
                echo -e "  ${YELLOW}⚠ Docker access enabled (User Namespace disabled)${NC}"
            else
                echo "  ✓ User Namespace (noroot)"
            fi
            echo ""
            echo -e "${GREEN}Security Level: 8/10${NC}"
            echo "Good for: Most use cases, excellent security/usability balance"
            ;;
        container)
            echo -e "${CYAN}Container + User Namespace Isolation (Maximum)${NC}"
            echo "  ✓ All namespace isolation features"
            echo "  ✓ Rootless container (podman)"
            echo "  ✓ Complete filesystem isolation"
            echo "  ✓ Immutable base image"
            echo "  ✓ Resource limits (cgroups)"
            echo "  ✓ SELinux/AppArmor integration"
            echo ""
            echo -e "${CYAN}Security Level: 9/10${NC}"
            echo "Good for: Maximum security, untrusted code"
            echo ""
            echo -e "${YELLOW}Note:${NC} Container mode requires podman and image build"
            ;;
    esac

    echo ""
    echo -e "${BLUE}Threat Protection:${NC}"

    case $ISOLATION_MODE in
        user)
            echo "  File access outside project:  Protected (user permissions)"
            echo "  Network exfiltration:         Vulnerable"
            echo "  Privilege escalation:         Limited protection"
            echo "  Process interference:         Limited protection"
            echo "  Device access:                Vulnerable"
            ;;
        namespace)
            echo "  File access outside project:  Hardened (explicit mounts)"
            echo "  Network exfiltration:         $([ "$ALLOW_NETWORK" = true ] && echo "Vulnerable" || echo "Blocked")"
            echo "  Privilege escalation:         Blocked"
            echo "  Process interference:         Blocked (PID namespace)"
            echo "  Device access:                Blocked"
            ;;
        container)
            echo "  File access outside project:  Completely isolated"
            echo "  Network exfiltration:         Configurable"
            echo "  Privilege escalation:         Blocked (multiple layers)"
            echo "  Process interference:         Completely isolated"
            echo "  Device access:                Completely isolated"
            ;;
    esac

    echo ""
    exit 0
}

# Build env args array for session environment variables
# Sets CLSECURE_SESSION and GH_TOKEN when available
_build_session_env() {
    SESSION_ENV_ARGS=()
    if [ -n "${SESSION_NAME:-}" ]; then
        SESSION_ENV_ARGS+=(CLSECURE_SESSION="$SESSION_NAME")
    fi

    local gh_token_val="${GH_TOKEN:-}"
    if [ -z "$gh_token_val" ] && command -v gh &>/dev/null; then
        gh_token_val=$(gh auth token 2>/dev/null || echo "")
    fi
    if [ -n "$gh_token_val" ]; then
        SESSION_ENV_ARGS+=(GH_TOKEN="$gh_token_val")
    fi
}

# Start user isolation session
start_user_session() {
    local continue_flag="$1"
    _build_session_env
    sudo -u "$WORKER_USER" env "${SESSION_ENV_ARGS[@]}" bash -c "cd && source ~/.bashrc && cd '$WORKER_PROJECT' && $CLAUDE_BIN --dangerously-skip-permissions $continue_flag"
}

# Start namespace isolation session (firejail)
start_namespace_session() {
    local continue_flag="$1"
    _build_session_env

    # Enhanced: Add firejail namespace isolation
    local network_flag=""
    [ "$ALLOW_NETWORK" = false ] && network_flag="--net=none"

    # Docker support
    local docker_flags="--noroot"
    if [ "$ALLOW_DOCKER" = true ]; then
        # Docker access requires disabling User Namespace (--noroot) to preserve group permissions
        # and ensuring the socket is accessible
        docker_flags="--noblacklist=/var/run/docker.sock --noblacklist=/run/docker.sock"
    fi

    sudo -u "$WORKER_USER" env "${SESSION_ENV_ARGS[@]}" bash -c "cd && source ~/.bashrc && cd '$WORKER_PROJECT' && firejail --quiet --noprofile --allusers --read-only=/home/linuxbrew $network_flag --private-dev --private-tmp $docker_flags --caps.drop=all --seccomp --deterministic-shutdown -- $CLAUDE_BIN --dangerously-skip-permissions $continue_flag"
}

# Start container isolation session (podman)
start_container_session() {
    local continue_flag="$1"
    # Maximum security: Podman rootless container
    log_error "Container mode not yet implemented in this prototype"
    log_info "Use --mode namespace for enhanced isolation"
    return 1
}

# Start shell session (user isolation, no Claude)
start_user_shell() {
    _build_session_env
    log_info "Starting shell as $WORKER_USER..."
    sudo -u "$WORKER_USER" env "${SESSION_ENV_ARGS[@]}" bash -c "cd && source ~/.bashrc && cd '$WORKER_PROJECT' && exec bash -l"
}

# Start shell session (namespace isolation, no Claude)
start_namespace_shell() {
    _build_session_env

    local network_flag=""
    [ "$ALLOW_NETWORK" = false ] && network_flag="--net=none"

    local docker_flags="--noroot"
    if [ "$ALLOW_DOCKER" = true ]; then
        docker_flags="--noblacklist=/var/run/docker.sock --noblacklist=/run/docker.sock"
    fi

    log_info "Starting shell in firejail namespace..."
    sudo -u "$WORKER_USER" env "${SESSION_ENV_ARGS[@]}" bash -c "cd && source ~/.bashrc && cd '$WORKER_PROJECT' && firejail --quiet --noprofile --allusers --read-only=/home/linuxbrew $network_flag --private-dev --private-tmp $docker_flags --caps.drop=all --seccomp --deterministic-shutdown -- bash -l"
}


# --- Module: sync.sh ---
# lib/sync.sh
# 
# Sync-back logic for importing changes from worker to main repository
# 
# Dependencies: lib/logging.sh, lib/git.sh, lib/worker.sh, lib/vars.sh
# Exports: detect_worker_changes, import_commits, import_uncommitted_changes, create_branch_and_import, show_sync_summary
# 
# Usage:
#   source lib/sync.sh
#   detect_worker_changes

# Detect changes in worker repository (commits and uncommitted)
# Sets global variables: WORKER_COMMITS, NUM_COMMITS, WORKER_CHANGES
detect_worker_changes() {
    # 1. Check for COMMITS
    # We compare HEAD against the original branch we cloned from.
    # Since we cloned the current dir, 'origin' in the worker points to here.
    # First, fetch to ensure worker's view of origin is up-to-date
    sudo -u "$WORKER_USER" git -C "$WORKER_PROJECT" fetch origin 2>/dev/null || true

    WORKER_COMMITS=$(sudo -u "$WORKER_USER" git -C "$WORKER_PROJECT" log --oneline origin/$ORIGINAL_BRANCH..HEAD 2>/dev/null || echo "")
    # Count commits using grep -c (more accurate than wc -l which counts newlines)
    NUM_COMMITS=$(echo "$WORKER_COMMITS" | grep -c . 2>/dev/null || echo 0)

    # 2. Check for UNCOMMITTED changes
    WORKER_CHANGES=$(sudo -u "$WORKER_USER" bash -c "cd '$WORKER_PROJECT' && git status --porcelain" 2>/dev/null || echo "")
}

# Show summary of detected changes
show_sync_summary() {
    if [ -z "$WORKER_CHANGES" ] && [ "$NUM_COMMITS" -eq 0 ]; then
        log_info "No changes detected (committed or uncommitted)."
        return 0
    fi

    echo "Changes detected:"
    if [ "$NUM_COMMITS" -gt 0 ]; then
        echo -e "${CYAN}$NUM_COMMITS new commit(s):${NC}"
        echo "$WORKER_COMMITS" | head -10
        [ "$NUM_COMMITS" -gt 10 ] && echo "... and more"
    fi

    if [ -n "$WORKER_CHANGES" ]; then
        echo -e "${CYAN}Uncommitted changes:${NC}"
        echo "$WORKER_CHANGES" | head -10
        local change_count=$(echo "$WORKER_CHANGES" | grep -c . 2>/dev/null || echo 0)
        [ "$change_count" -gt 10 ] && echo "... and more"
    fi
    echo ""
    return 1
}

# Import commits from worker repository
import_commits() {
    if [ "$NUM_COMMITS" -eq 0 ]; then
        return 0
    fi

    log_info "Importing $NUM_COMMITS commit(s) from worker..."
    
    # Ensure we can read the worker's git objects
    # We grant read access to others temporarily for the .git directory
    # Store original directory permissions to restore later (only for .git directory itself)
    local old_git_dir_perms=$(stat -c "%a" "$WORKER_PROJECT/.git" 2>/dev/null || echo "")
    sudo chmod -R o+rX "$WORKER_PROJECT/.git"
    
    # Stash any local changes (including untracked files) to avoid merge conflicts
    local stash_created=false
    if ! git diff --quiet || ! git diff --cached --quiet || [ -n "$(git ls-files --others --exclude-standard)" ]; then
        log_info "Stashing local changes (including untracked files)..."
        git stash push -u -m "clsecure: backup before importing worker commits"
        stash_created=true
        log_info "✓ Local changes stashed."
    fi
    
    # Fetch and merge
    # We fetch HEAD from worker and merge it. 
    # Since we just created the branch from the same base, this should be a fast-forward or clean merge.
    if git pull --no-edit "$WORKER_PROJECT" HEAD; then
        log_info "✓ Commits imported successfully."
        
        # Restore stashed changes if any
        if [ "$stash_created" = true ]; then
            echo ""
            log_info "Restoring stashed changes..."
            if git stash pop; then
                log_info "✓ Stashed changes restored."
            else
                log_warn "Conflicts detected when restoring stashed changes."
                log_warn "Please resolve conflicts manually and run: git stash drop"
            fi
        fi
    else
        log_error "Failed to import commits."
        
        # Restore stash on failure
        if [ "$stash_created" = true ]; then
            log_info "Restoring stashed changes..."
            git stash pop
        fi
        return 1
    fi
    
    # Restore original git permissions for security
    # Restore directory permissions to .git directory itself, then remove world access from contents
    if [ -n "$old_git_dir_perms" ]; then
        # Restore directory permissions to .git itself (not recursive)
        sudo chmod "$old_git_dir_perms" "$WORKER_PROJECT/.git" 2>/dev/null || true
        # Remove world read access from all files and directories inside .git
        # Use find to apply different permissions to files vs directories
        sudo find "$WORKER_PROJECT/.git" -type f -exec chmod o-r {} \; 2>/dev/null || true
        sudo find "$WORKER_PROJECT/.git" -type d -exec chmod o-rX {} \; 2>/dev/null || true
    else
        # Default to removing world read access if we don't know original perms
        # Remove world read from files, world read+execute from directories
        sudo find "$WORKER_PROJECT/.git" -type f -exec chmod o-r {} \; 2>/dev/null || true
        sudo find "$WORKER_PROJECT/.git" -type d -exec chmod o-rX {} \; 2>/dev/null || true
    fi
    
    return 0
}

# Import uncommitted changes from worker repository
import_uncommitted_changes() {
    if [ -z "$WORKER_CHANGES" ]; then
        return 0
    fi

    log_info "Syncing uncommitted changes..."
    sudo rsync -av \
        --exclude='.git' \
        --exclude='node_modules' \
        --exclude='venv' \
        --exclude='.venv' \
        --exclude='__pycache__' \
        --exclude='.pytest_cache' \
        --exclude='dist' \
        --exclude='build' \
        --exclude='.next' \
        --exclude='target' \
        "$WORKER_PROJECT/" "$CURRENT_DIR/"
    
    sudo chown -R "$(whoami):$(id -gn)" "$CURRENT_DIR"
    
    log_info "✓ Uncommitted changes applied."
    
    echo ""
    git status --short
    echo ""
    
    read -p "Commit these changes now? (y/n): " commit_now
    if [[ "$commit_now" =~ ^[Yy]$ ]]; then
         read -p "Commit message [WIP from Claude]: " commit_msg
         commit_msg=${commit_msg:-"WIP from Claude"}
         git add -A
         git commit -m "$commit_msg"
         log_info "✓ Committed."
    fi
    
    return 0
}

# Create branch and import all work (commits + changes)
create_branch_and_import() {
    local timestamp=$(date +%Y%m%d-%H%M%S)
    local default_branch
    if [ -n "${SESSION_NAME_SANITIZED:-}" ]; then
        default_branch="claude/${SAFE_PROJECT_NAME}-${SESSION_NAME_SANITIZED}-${timestamp}"
    else
        default_branch="claude/${SAFE_PROJECT_NAME}-${timestamp}"
    fi
    
    read -p "Branch name [$default_branch]: " branch_name
    branch_name=${branch_name:-$default_branch}
    
    # Use bash built-in pattern matching instead of echo | grep (more efficient)
    if [[ ! "$branch_name" =~ ^[a-zA-Z0-9/_-]+$ ]]; then
        log_error "Invalid branch name."
        return 1
    fi
    
    if git show-ref --verify --quiet "refs/heads/$branch_name" 2>/dev/null; then
        log_error "Branch already exists."
        return 1
    fi
    
    log_info "Creating branch '$branch_name'..."
    git checkout -b "$branch_name"
    
    # Import Commits
    if ! import_commits; then
        return 1
    fi
    
    # Sync Uncommitted Changes
    import_uncommitted_changes
    
    # Push / PR logic (simplified from original)
    echo ""
    read -p "Push branch '$branch_name'? (y/n): " push_now
    if [[ "$push_now" =~ ^[Yy]$ ]]; then
        git push -u origin "$branch_name"
    fi
    
    echo ""
    read -p "Switch back to '$ORIGINAL_BRANCH'? (y/n): " switch
    [[ "$switch" =~ ^[Yy]$ ]] && git checkout "$ORIGINAL_BRANCH"
    
    echo ""
    read -p "Remove worker user '$WORKER_USER'? (y/n): " cleanup
    if [[ "$cleanup" =~ ^[Yy]$ ]]; then
        sudo userdel -r "$WORKER_USER" 2>/dev/null || true
        log_info "User removed."
    fi
    
    return 0
}


# --- Module: cleanup.sh ---
# lib/cleanup.sh
#
# Session cleanup for clsecure (application hooks + OS-level process termination)
#
# Dependencies: lib/logging.sh, lib/worker.sh, lib/vars.sh
# Exports: cleanup_session, validate_cleanup_hook, run_cleanup_hook, auto_detect_docker_cleanup, kill_worker_processes
#
# Usage:
#   source lib/cleanup.sh
#   cleanup_session "stop"    # Normal cleanup (keep data volumes)
#   cleanup_session "purge"   # Full cleanup (destroy data volumes)

# Top-level cleanup orchestrator
# Args: cleanup_level ("stop" or "purge")
cleanup_session() {
    local cleanup_level="${1:-stop}"

    # Guard against unset worker variables
    if [ -z "${WORKER_USER:-}" ] || [ -z "${WORKER_PROJECT:-}" ]; then
        log_warn "Cannot run cleanup: worker variables not set"
        return 0
    fi

    log_step "Cleaning up session resources..."

    # Tier 1: Application hook (if exists and valid)
    if validate_cleanup_hook; then
        run_cleanup_hook "$cleanup_level"
    elif [ "$ALLOW_DOCKER" = true ] && [ "$SKIP_DOCKER_AUTODETECT" = false ]; then
        auto_detect_docker_cleanup "$cleanup_level"
    fi

    # Tier 2: OS-level process kill (unconditional)
    kill_worker_processes
}

# Validate the project cleanup hook
# Returns 0 if hook is valid and should be run, 1 otherwise
validate_cleanup_hook() {
    local hook_path="$WORKER_PROJECT/.clsecure/on-cleanup"

    # Must exist as a regular file
    if ! sudo test -f "$hook_path"; then
        return 1
    fi

    # Must be executable
    if ! sudo test -x "$hook_path"; then
        log_warn "Cleanup hook exists but is not executable: $hook_path"
        return 1
    fi

    # Must not symlink outside project directory
    local resolved_path
    resolved_path=$(sudo readlink -f "$hook_path" 2>/dev/null || echo "")
    if [ -z "$resolved_path" ]; then
        log_warn "Cannot resolve cleanup hook path"
        return 1
    fi

    # Ensure resolved path is within the worker project
    case "$resolved_path" in
        "$WORKER_PROJECT"/*)
            return 0
            ;;
        *)
            log_warn "Cleanup hook symlinks outside project directory: $resolved_path"
            return 1
            ;;
    esac
}

# Run the project cleanup hook as the worker user
# Args: cleanup_level ("stop" or "purge")
run_cleanup_hook() {
    local cleanup_level="${1:-stop}"
    local hook_path="$WORKER_PROJECT/.clsecure/on-cleanup"

    log_info "Running project cleanup hook..."

    local exit_code=0
    sudo -u "$WORKER_USER" \
        env \
            CLSECURE_SESSION="${SESSION_NAME:-}" \
            CLSECURE_CLEANUP_LEVEL="$cleanup_level" \
            CLSECURE_PROJECT_DIR="$WORKER_PROJECT" \
            CLSECURE_WORKER_USER="$WORKER_USER" \
            CLSECURE_WORKER_HOME="$WORKER_HOME" \
        timeout "$CLEANUP_HOOK_TIMEOUT" \
        "$hook_path" || exit_code=$?

    if [ "$exit_code" -eq 124 ]; then
        log_warn "Cleanup hook timed out after ${CLEANUP_HOOK_TIMEOUT}s"
    elif [ "$exit_code" -ne 0 ]; then
        log_warn "Cleanup hook exited with code $exit_code"
    fi

    # Always continue to Tier 2 regardless of hook result
    return 0
}

# Auto-detect and clean up docker compose resources
# Fallback when no cleanup hook exists but docker is enabled
# Args: cleanup_level ("stop" or "purge")
auto_detect_docker_cleanup() {
    local cleanup_level="${1:-stop}"

    # Check if docker is available
    if ! command -v docker &>/dev/null; then
        return 0
    fi

    # Search for compose file in project directory
    local compose_file=""
    for candidate in docker-compose.yml docker-compose.yaml compose.yml compose.yaml; do
        if sudo test -f "$WORKER_PROJECT/$candidate"; then
            compose_file="$candidate"
            break
        fi
    done

    if [ -z "$compose_file" ]; then
        return 0
    fi

    log_info "Auto-detected $compose_file, running docker compose down..."

    local docker_args="down"
    if [ "$cleanup_level" = "purge" ]; then
        docker_args="down -v --remove-orphans"
    fi

    sudo -u "$WORKER_USER" bash -c "cd '$WORKER_PROJECT' && docker compose $docker_args" || {
        log_warn "Docker compose cleanup failed (non-fatal)"
    }

    return 0
}

# Kill all processes owned by the worker user (Tier 2 brute force)
kill_worker_processes() {
    # Check if any processes exist
    if ! sudo pgrep -u "$WORKER_USER" &>/dev/null; then
        return 0
    fi

    log_info "Terminating processes for $WORKER_USER..."

    # SIGTERM first
    sudo pkill -TERM -u "$WORKER_USER" 2>/dev/null || true

    # Wait up to 5 seconds
    local wait_count=0
    while sudo pgrep -u "$WORKER_USER" &>/dev/null && [ "$wait_count" -lt 10 ]; do
        sleep 0.5
        wait_count=$((wait_count + 1))
    done

    # SIGKILL remaining
    if sudo pgrep -u "$WORKER_USER" &>/dev/null; then
        log_warn "Sending SIGKILL to remaining processes..."
        sudo pkill -KILL -u "$WORKER_USER" 2>/dev/null || true
        sleep 1
    fi
}


# Initialize all global variables
init_clsecure_vars


# ============================================================================
# Main Script (orchestration, CLI parsing, execution flow)
# ============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LIB_DIR="${SCRIPT_DIR}/lib"

# Phase 1: Initialize variables FIRST (before sourcing other modules)

# Phase 2: Source modules in dependency order
# Level 1: No dependencies (but uses vars for colors)

# Level 2: Depends on logging + vars

# Level 3: Depends on logging + config/vars

# Level 4: Depends on logging + worker + vars

# Level 5: Depends on logging + git + worker + vars

# Level 6: Depends on logging + worker + vars


show_help() {
    cat << EOF
Usage: clsecure [OPTIONS]

Run Claude Code with enhanced isolation (User + Namespace).

OPTIONS:
    --help, -h          Show this help message
    --list, -l          List all claude-worker users and their status
    --cleanup           Interactively remove worker users
    --cleanup-all       Remove ALL claude-worker users (requires confirmation)
    --mode MODE         Isolation mode: user, namespace (default), container
    --allow-network     Allow network access (default: from config or true)
    --no-network        Disable network access (--net=none in firejail)
    --allow-docker      Allow access to Docker (WARNING: reduces isolation)
    --no-docker         Disable Docker access (default)
    --install-deps      Install project dependencies (npm/pip) on startup
    --config            Show current configuration and config file location
    --info              Show isolation details for current mode
    --shell             Start shell instead of Claude (for debugging)
    --skip-setup        Skip setup script execution
    --full-clone        Clone full git history (slower, default is shallow)
    --session, -s NAME  Run a named session (multiple environments per project)

ISOLATION MODES:
    user                Basic user isolation (original clsecure behavior)
    namespace           User + Namespace (firejail) - RECOMMENDED
    container           User + Container (podman rootless) - MAXIMUM SECURITY

CONFIGURATION FILE:
    Project:  .clsecure/config (per-project, shareable via git)
    User:     ~/.config/clsecure/config or ~/.clsecurerc

    Precedence: CLI args > User config > Project config > Defaults

    Security-sensitive settings (docker, network, setup_script,
    install_dependencies) can only be set via user config or CLI flags.
    Project config can set: mode, cleanup_hook_timeout, skip_docker_autodetect.

REQUIREMENTS:
    - git (must be run from a git repository)
    - rsync
    - sudo privileges
    - For namespace mode: firejail
    - For container mode: podman

EXAMPLES:
    # Run with recommended namespace isolation
    clsecure

    # Run with network disabled
    clsecure --no-network

    # Run with Docker access (for docker-compose)
    clsecure --allow-docker

    # Run with basic user isolation only
    clsecure --mode user

    # Show config file and current settings
    clsecure --config

    # Show isolation details
    clsecure --info

    # Run multiple sessions for the same project
    clsecure --session auth
    clsecure --session payments

SECURITY MODEL:
    User isolation:      Dedicated Linux user per project
    Namespace isolation: + Firejail sandbox (PID, mount, network, IPC)
    Container isolation: + Full containerization (podman rootless)

EOF
    exit 0
}



check_requirements() {
    local missing=()

    command -v git &>/dev/null || missing+=("git")
    command -v rsync &>/dev/null || missing+=("rsync")
    command -v sudo &>/dev/null || missing+=("sudo")

    if [ ${#missing[@]} -ne 0 ]; then
        log_error "Missing required tools: ${missing[*]}"
        exit 1
    fi

    if ! sudo -n true 2>/dev/null; then
        log_warn "This script requires sudo privileges."
        sudo true || { log_error "Failed to obtain sudo privileges."; exit 1; }
    fi
}

# Note: Trap handler will be registered after lock acquisition (line ~273)
# This ensures cleanup happens even on early exits (e.g., missing git repo, missing Claude CLI)
# The trap must be registered AFTER acquire_lock succeeds, so the lock is released on exit

# ------------------------------------------------------------------------------
# Load configuration file (before parsing CLI args)
# ------------------------------------------------------------------------------
load_config

# ------------------------------------------------------------------------------
# Parse arguments (CLI overrides config file)
# ------------------------------------------------------------------------------
while [[ $# -gt 0 ]]; do
    case $1 in
        --help|-h)
            show_help
            ;;
        --list|-l)
            list_workers
            ;;
        --cleanup)
            cleanup_workers
            ;;
        --cleanup-all)
            cleanup_all_workers
            ;;
        --info)
            show_isolation_info
            ;;
        --config)
            show_config_info
            ;;
        --mode)
            ISOLATION_MODE="$2"
            if [[ ! "$ISOLATION_MODE" =~ ^(user|namespace|container)$ ]]; then
                log_error "Invalid mode: $ISOLATION_MODE (must be user, namespace, or container)"
                exit 1
            fi
            shift 2
            ;;
        --allow-network)
            ALLOW_NETWORK=true
            shift
            ;;
        --no-network)
            ALLOW_NETWORK=false
            shift
            ;;
        --allow-docker)
            ALLOW_DOCKER=true
            shift
            ;;
        --no-docker)
            ALLOW_DOCKER=false
            shift
            ;;
        --install-deps)
            INSTALL_DEPS=true
            shift
            ;;
        --shell)
            SHELL_ONLY=true
            shift
            ;;
        --skip-setup)
            SKIP_SETUP=true
            shift
            ;;
        --full-clone)
            FULL_CLONE=true
            shift
            ;;
        --session|-s)
            SESSION_NAME="$2"
            shift 2
            ;;
        *)
            log_error "Unknown option: $1"
            show_help
            ;;
    esac
done

# Recompute worker variables if a session name was provided
if [ -n "$SESSION_NAME" ]; then
    if ! recompute_worker_vars; then
        log_error "Invalid session name: '$SESSION_NAME' (sanitizes to empty)"
        exit 1
    fi
fi

# ------------------------------------------------------------------------------
# Main script
# ------------------------------------------------------------------------------
echo ""
echo -e "${GREEN}╔════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║  Claude Code Enhanced Secure Runner    ║${NC}"
echo -e "${GREEN}╚════════════════════════════════════════╝${NC}"
echo ""

# Check requirements
log_step "Checking requirements..."
check_requirements
check_isolation_requirements

# Verify git repository
log_step "Verifying git repository..."

if [ ! -d ".git" ]; then
    log_error "Not a git repository. Please run from a git project root."
    exit 1
fi

if ! git diff-index --quiet HEAD -- 2>/dev/null; then
    log_warn "You have uncommitted changes in your working directory."
    read -p "Continue anyway? (y/n): " continue_anyway
    if [[ ! "$continue_anyway" =~ ^[Yy]$ ]]; then
        log_info "Aborted. Please commit or stash your changes first."
        exit 0
    fi
fi

ORIGINAL_BRANCH=$(git branch --show-current)
if [ -z "$ORIGINAL_BRANCH" ]; then
    ORIGINAL_BRANCH=$(git rev-parse --short HEAD)
    log_warn "Detached HEAD state. Using commit: $ORIGINAL_BRANCH"
fi
export ORIGINAL_BRANCH  # Export so it's available in sync.sh functions

# Warn if on a claude/* branch (temporary branch from previous session)
if [[ "$ORIGINAL_BRANCH" =~ ^claude/ ]]; then
    log_warn "You are currently on a temporary Claude branch: $ORIGINAL_BRANCH"
    log_warn "This may prevent proper commit import from the worker session."
    echo ""
    echo "It's recommended to switch to your main branch first, for example:"
    echo "  git checkout main    # or develop, master, etc."
    echo ""
    read -p "Continue anyway? (y/n): " CONTINUE_ANYWAY
    if [[ ! "$CONTINUE_ANYWAY" =~ ^[Yy]$ ]]; then
        log_info "Aborted. Please switch to your main branch first."
        exit 0
    fi
fi

log_info "Project: $PROJECT_NAME"
log_info "Branch: $ORIGINAL_BRANCH"
[ -n "$SESSION_NAME" ] && log_info "Session: $SESSION_NAME"
log_info "Worker user: $WORKER_USER"
log_security "Isolation: $ISOLATION_MODE$([ "$ALLOW_NETWORK" = true ] && echo " (network enabled)" || echo " (network disabled)")$([ "$ALLOW_DOCKER" = true ] && echo " (docker enabled)")"

# Check if already running
log_step "Checking for existing session..."

if ! acquire_lock; then
    if [ -n "$SESSION_NAME" ]; then
        log_error "A session for '$PROJECT_NAME' (session: $SESSION_NAME) is already running!"
    else
        log_error "A session for '$PROJECT_NAME' is already running!"
    fi
    log_info "Use 'clsecure --list' to see active sessions."
    exit 1
fi

# Register trap handler early (after lock acquisition) to ensure cleanup on early exits
# This is critical: if script exits early (e.g., missing git repo, missing Claude CLI),
# the trap must be registered to release the lock
trap cleanup_on_exit EXIT

# Create worker user if needed
log_step "Setting up worker user '$WORKER_USER'..."
create_worker_user

# Handle existing project files
SKIP_COPY=false

if [ -d "$WORKER_PROJECT" ]; then
    echo ""
    log_warn "Project files already exist for this user."
    echo ""
    echo "Options:"
    echo "  1) Resume (keep existing files)"
    echo "  2) Replace (fresh copy from source)"
    echo "  3) Abort"
    echo ""
    read -p "Choose (1/2/3): " EXISTING_OPTION

    case $EXISTING_OPTION in
        1)
            log_info "Resuming..."
            SKIP_COPY=true
            ;;
        2)
            log_info "Replacing..."
            sudo rm -rf "$WORKER_PROJECT"
            ;;
        3)
            log_info "Aborted."
            release_lock
            exit 0
            ;;
        *)
            log_error "Invalid option."
            release_lock
            exit 1
            ;;
    esac
fi

# Clone repository and sync working directory
if [ "$SKIP_COPY" = false ]; then
    log_step "Setting up project files..."

    # Check available disk space before cloning
    REQUIRED_SPACE_MB=1000  # Estimate: 1GB minimum
    if ! check_disk_space "$REQUIRED_SPACE_MB"; then
        release_lock
        exit 1
    fi

    if ! clone_repository; then
        release_lock
        exit 1
    fi

    sync_working_directory
    copy_submodules

    log_info "Project ready at $WORKER_PROJECT"
fi

setup_worker_home

# Make sure project-local MCP config is portable across worker users
sanitize_mcp_config

# Copy configurations
log_step "Setting up configurations..."

if [ -d "$HOME/.claude" ]; then
    log_info "Copying Claude config (excluding large files)..."
    sudo mkdir -p "$WORKER_HOME/.claude"
    # Use rsync to exclude large unnecessary files for faster copy
    sudo rsync -a \
        --exclude='debug' \
        --exclude='file-history' \
        --exclude='history.jsonl' \
        --exclude='cache' \
        --exclude='paste-cache' \
        --exclude='plans' \
        --exclude='__store.db' \
        --exclude='*.db' \
        --exclude='*.log' \
        --exclude='venv' \
        --exclude='.venv' \
        --exclude='__pycache__' \
        --exclude='.git' \
        --exclude='node_modules' \
        "$HOME/.claude/" "$WORKER_HOME/.claude/" 2>/dev/null || true
    sudo chown -R "$WORKER_USER:$WORKER_USER" "$WORKER_HOME/.claude"
    sanitize_worker_claude_home_paths
fi

setup_git_config

# Setup shell environment
log_step "Setting up shell environment..."

WORKER_BASHRC="$WORKER_HOME/.bashrc"

if ! grep -q "NPM_CONFIG_PREFIX" "$WORKER_BASHRC" 2>/dev/null; then
    sudo tee "$WORKER_BASHRC" > /dev/null << 'BASHRCEOF'
# Linuxbrew setup
if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then
    eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
fi

# NPM global packages
export NPM_CONFIG_PREFIX="$HOME/.npm-global"
export PATH="$HOME/.npm-global/bin:$PATH"
BASHRCEOF
    sudo chown "$WORKER_USER:$WORKER_USER" "$WORKER_BASHRC"
    log_info "Shell environment configured."
else
    log_info "Shell environment already configured."
fi

sudo -u "$WORKER_USER" mkdir -p "$WORKER_HOME/.npm-global"

# Warn early if MCP runtime is missing
check_worker_mcp_runtime

# Verify Claude CLI
log_step "Checking Claude CLI..."

if [ -x "/home/linuxbrew/.linuxbrew/bin/claude" ]; then
    log_info "Claude CLI found."
    CLAUDE_BIN="/home/linuxbrew/.linuxbrew/bin/claude"
    export CLAUDE_BIN  # Export so it's available in subshells (isolation.sh functions)
else
    log_error "Claude CLI not found at /home/linuxbrew/.linuxbrew/bin/claude"
    log_info "Install with: brew install claude-code"
    release_lock
    exit 1
fi

# Install task-master-ai (with npm cache copy and retry)
install_task_master 2

# Run setup script if configured (optional - failures should not terminate script)
if [ "$SKIP_SETUP" = true ]; then
    log_info "Skipping setup script (--skip-setup)"
else
    run_setup_script || {
        log_warn "Setup script execution failed, but continuing..."
    }
fi

# Install dependencies if requested
if [ "$INSTALL_DEPS" = true ]; then
    install_project_dependencies
fi


# ------------------------------------------------------------------------------
# Start session with selected isolation
# ------------------------------------------------------------------------------
echo ""
if [ "$SHELL_ONLY" = true ]; then
    echo -e "${CYAN}╔════════════════════════════════════════╗${NC}"
    echo -e "${CYAN}║     Starting Debug Shell Session       ║${NC}"
    echo -e "${CYAN}╚════════════════════════════════════════╝${NC}"
else
    echo -e "${GREEN}╔════════════════════════════════════════╗${NC}"
    echo -e "${GREEN}║     Starting Claude Code Session       ║${NC}"
    echo -e "${GREEN}╚════════════════════════════════════════╝${NC}"
fi
echo ""
log_info "User: $WORKER_USER"
log_info "Project: $WORKER_PROJECT"
log_security "Isolation: $ISOLATION_MODE"
echo ""

# Add --continue flag if resuming an existing session
CONTINUE_FLAG=""
if [ "$SKIP_COPY" = true ]; then
    CONTINUE_FLAG="--continue"
    log_info "Resuming previous Claude session..."
fi

if [ "$SHELL_ONLY" = true ]; then
    echo -e "${YELLOW}Type 'exit' to end shell session.${NC}"
else
    echo -e "${YELLOW}Type /exit or press Ctrl+C to end session.${NC}"
fi
echo ""

set +e

if [ "$SHELL_ONLY" = true ]; then
    # Shell-only mode for debugging
    case $ISOLATION_MODE in
        user)
            start_user_shell
            ;;
        namespace)
            start_namespace_shell
            ;;
        container)
            log_error "Shell mode not implemented for container isolation"
            release_lock
            exit 1
            ;;
    esac
else
    # Normal Claude session
    case $ISOLATION_MODE in
        user)
            start_user_session "$CONTINUE_FLAG"
            ;;
        namespace)
            start_namespace_session "$CONTINUE_FLAG"
            ;;
        container)
            if ! start_container_session "$CONTINUE_FLAG"; then
                release_lock
                exit 1
            fi
            ;;
    esac
fi

CLAUDE_EXIT_CODE=$?
# Keep set +e for the change detection phase to avoid premature exits
set +e

# ------------------------------------------------------------------------------
# Handle changes (Commits + Uncommitted)
# ------------------------------------------------------------------------------
echo ""
echo -e "${GREEN}╔════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║     Session Ended                      ║${NC}"
echo -e "${GREEN}╚════════════════════════════════════════╝${NC}"
echo ""

# Detect changes (sets WORKER_COMMITS, NUM_COMMITS, WORKER_CHANGES)
set +e  # Temporarily disable strict error handling for change detection
detect_worker_changes
set -e  # Re-enable strict error handling after safely gathering change information

if show_sync_summary; then
    # No changes detected
    echo ""
    read -p "Remove worker user '$WORKER_USER'? (y/n): " cleanup
    if [[ "$cleanup" =~ ^[Yy]$ ]]; then
        cleanup_session "stop"
        sudo userdel -r "$WORKER_USER" 2>/dev/null || true
        log_info "User removed."
    fi
    release_lock
    exit 0
fi

echo "Options:"
echo "  1) Create branch and import ALL work (commits + changes)"
echo "  2) Discard changes and remove user"
echo "  3) Keep for later (resume with clsecure)"
echo ""
read -p "Choose (1/2/3): " OPTION

case $OPTION in
    1)
        if ! create_branch_and_import; then
            exit 1
        fi
        cleanup_session "stop"
        ;;
    2)
        cleanup_level="stop"
        if [ "$ALLOW_DOCKER" = true ]; then
            echo ""
            read -p "Also remove Docker data volumes? (y/n): " purge_volumes
            [[ "$purge_volumes" =~ ^[Yy]$ ]] && cleanup_level="purge"
        fi
        cleanup_session "$cleanup_level"
        log_warn "Removing user and all changes..."
        sudo userdel -r "$WORKER_USER" 2>/dev/null || true
        log_info "Done."
        ;;
    3)
        log_info "Keeping user '$WORKER_USER' for later."
        echo "Resume by running 'clsecure' from the same project."
        ;;
esac

release_lock
echo ""
log_info "Done!"

# Register trap handler after all initialization
trap cleanup_on_exit EXIT
